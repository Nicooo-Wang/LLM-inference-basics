# PagedAttention ä¸å†…å­˜ç®¡ç†

---

## ä¸€ã€å¼€ç¯‡å¼•å…¥

### 1.1 KV Cache çš„å†…å­˜ç“¶é¢ˆ

åœ¨ Transformer æ¶æ„çš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ¯ç”Ÿæˆä¸€ä¸ªæ–° token éƒ½éœ€è¦ç”¨åˆ°ä¹‹å‰æ‰€æœ‰ token çš„ Key å’Œ Value å‘é‡ã€‚ä¸ºé¿å…é‡å¤è®¡ç®—ï¼Œæˆ‘ä»¬ä¼šå°†è¿™äº› KV å‘é‡ç¼“å­˜èµ·æ¥ï¼Œè¿™å°±æ˜¯ **KV Cache**ã€‚

KV Cache çš„æ˜¾å­˜å ç”¨å¯ä»¥ç”¨ä»¥ä¸‹å…¬å¼ä¼°ç®—ï¼š

```
KV Cache Size = 2 Ã— num_layers Ã— seq_len Ã— num_kv_heads Ã— head_dim Ã— dtype_size
```

ä»¥ Qwen3-0.6B ä¸ºä¾‹ï¼ˆ28å±‚ï¼Œ8ä¸ªKVå¤´ï¼Œ64ç»´ï¼Œbf16ï¼‰ï¼š
- å•ä¸ªè¯·æ±‚ã€åºåˆ—é•¿åº¦ 4096ï¼š`2 Ã— 28 Ã— 4096 Ã— 8 Ã— 64 Ã— 2 = 234 MB`
- å¦‚æœåŒæ—¶æœåŠ¡ 32 ä¸ªè¯·æ±‚ï¼šçº¦ **7.34 GB** æ˜¾å­˜ä»…ç”¨äº KV Cache

ä¼ ç»Ÿæ–¹æ¡ˆçš„é—®é¢˜åœ¨äº **é¢„åˆ†é…æœ€å¤§é•¿åº¦**ã€‚å‡è®¾ `max_model_len = 4096`ï¼Œå³ä½¿ä¸€ä¸ªè¯·æ±‚å®é™…åªç”Ÿæˆ 100 ä¸ª tokenï¼Œä¹Ÿä¼šé¢„åˆ†é… 4096 é•¿åº¦çš„ KV Cache ç©ºé—´ï¼Œå¯¼è‡´ **60%-80% çš„æ˜¾å­˜æµªè´¹**ã€‚æ­¤å¤–ï¼Œä¸åŒè¯·æ±‚çš„å®é™…é•¿åº¦å‚å·®ä¸é½ï¼Œå®¹æ˜“äº§ç”Ÿ **æ˜¾å­˜ç¢ç‰‡**ã€‚
![pic](https://cdn.prod.website-files.com/618399cd49d125734c8dec95/663e10712d1f683554c11f56_UubwS2ISHuve8WjvuB6QaIzh9MZzmLNC5Rz4EhJ03CS_6u9KrhRMkWHexslnmdTYCCjEp07aqg3sBmQy-63FxZnSNUTYOr1V-7Dr53qq4PvpERJaxf3DAKU_lKIFSHxSl7BbZpZDKruOOYkNDft7YaQ.png)

### 1.2 PagedAttention æ ¸å¿ƒæ€æƒ³

PagedAttention å€Ÿé‰´äº†æ“ä½œç³»ç»Ÿè™šæ‹Ÿå†…å­˜ç®¡ç†çš„ **åˆ†é¡µæœºåˆ¶**ï¼š

| æ“ä½œç³»ç»Ÿæ¦‚å¿µ       | PagedAttention ç±»æ¯”                        |
| ------------------ | ------------------------------------------ |
| é¡µï¼ˆPageï¼‰         | Blockï¼ˆå›ºå®šå¤§å°çš„ KV Cache å—ï¼‰            |
| é¡µè¡¨ï¼ˆPage Tableï¼‰ | Block Tableï¼ˆé€»è¾‘ä½ç½®åˆ°ç‰©ç† Block çš„æ˜ å°„ï¼‰ |
| slot               | token çº§åˆ«çš„ç‰©ç†ä½ç½®æ˜ å°„                   |
| è¿›ç¨‹ï¼ˆProcessï¼‰    | Sequenceï¼ˆä¸€ä¸ªæ¨ç†è¯·æ±‚ï¼‰                   |

æ ¸å¿ƒä¼˜åŠ¿ï¼š
- **æŒ‰éœ€åˆ†é…**ï¼šåªä¸ºå®é™…ç”Ÿæˆçš„ token åˆ†é… Block
- **åŠ¨æ€ç®¡ç†**ï¼šè¯·æ±‚ç»“æŸåç«‹å³å›æ”¶ Block ä¾›å…¶ä»–è¯·æ±‚ä½¿ç”¨
- **æ”¯æŒå…±äº«**ï¼šç›¸åŒå‰ç¼€çš„è¯·æ±‚å¯å…±äº« Blockï¼ˆPrefix Cachingï¼‰

![pic](https://terenceli.github.io/assets/img/vllmpageattn/1.png)
---

## äºŒã€nano-vllm ä»£ç ç»“æ„æ€»è§ˆ

### 2.1 ç›¸å…³æ–‡ä»¶å®šä½

```
nanovllm/
â”œâ”€â”€ engine/
â”‚   â”œâ”€â”€ block_manager.py   # Block åˆ†é…ä¸å›æ”¶ã€Prefix Caching
â”‚   â”œâ”€â”€ sequence.py        # Sequence æ•°æ®ç»“æ„ï¼ˆå« block_tableï¼‰
â”‚   â”œâ”€â”€ scheduler.py       # è°ƒåº¦å™¨ï¼ˆè°ƒç”¨ BlockManagerï¼‰
â”‚   â””â”€â”€ model_runner.py    # KV Cache ç‰©ç†æ˜¾å­˜åˆ†é…ã€ä¸Šä¸‹æ–‡å‡†å¤‡
â”œâ”€â”€ layers/
â”‚   â””â”€â”€ attention.py       # Attention è®¡ç®— + KV Cache å­˜å–
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ context.py         # è¿è¡Œæ—¶ä¸Šä¸‹æ–‡ï¼ˆslot_mappingã€block_tablesç­‰ï¼‰
â””â”€â”€ config.py              # kvcache_block_size ç­‰é…ç½®
```

### 2.2 æ¨¡å—åä½œå…³ç³»å›¾

![å›¾ 0](../.assets/9f28c9fd08682b0ae892c33534320503b8a0ad180619a335e9100da41413ff1d.png)  


**æ ¸å¿ƒåä½œæµç¨‹**ï¼š

1. **Scheduler** å†³å®šå“ªäº› Sequence å‚ä¸æœ¬æ¬¡è¿­ä»£
2. **BlockManager** ä¸º Sequence åˆ†é…/å›æ”¶ Blockï¼Œæ›´æ–° Sequence çš„ `block_table`
3. **ModelRunner** è¯»å– Sequence çš„ `block_table`ï¼Œæ„é€  `slot_mapping` ç­‰ä¸Šä¸‹æ–‡ä¿¡æ¯
4. **Context** ä¿å­˜è¿è¡Œæ—¶ä¸Šä¸‹æ–‡ï¼Œä¾› Attention å±‚ä½¿ç”¨
5. **Attention** æ ¹æ®ä¸Šä¸‹æ–‡è¯»å†™ KV Cacheï¼Œå®Œæˆæ³¨æ„åŠ›è®¡ç®—

---

## ä¸‰ã€æ ¸å¿ƒç±»ä¸æ–¹æ³•è¯¦è§£

### 3.1 Block ç±»ï¼ˆblock_manager.pyï¼‰

Block æ˜¯ PagedAttention çš„æœ€å°å­˜å‚¨å•å…ƒï¼Œä»£è¡¨ KV Cache ä¸­çš„ä¸€ä¸ªå›ºå®šå¤§å°çš„æ§½ä½ã€‚

```python
class Block:
    """
    ç‰©ç† Blockï¼ŒKV Cache çš„æœ€å°å­˜å‚¨å•å…ƒã€‚
    
    æ¯ä¸ª Block å¯å­˜å‚¨ block_size ä¸ª token çš„ KV å‘é‡ã€‚
    é€šè¿‡ ref_count æ”¯æŒå¤šä¸ª Sequence å…±äº«åŒä¸€ Blockï¼ˆPrefix Cachingï¼‰ã€‚
    é€šè¿‡ hash å’Œ token_ids æ”¯æŒç¼“å­˜æŸ¥æ‰¾å’Œç¢°æ’æ ¡éªŒã€‚
    """

    def __init__(self, block_id):
        # block_id: ç‰©ç† Block çš„å”¯ä¸€æ ‡è¯†ï¼Œå¯¹åº” KV Cache Tensor çš„ç¬¬ 2 ç»´ç´¢å¼•
        # åˆ›å»ºåä¸å˜ï¼ŒèŒƒå›´æ˜¯ [0, num_kvcache_blocks)
        self.block_id = block_id
        
        # ref_count: å¼•ç”¨è®¡æ•°
        # = 0: Block ç©ºé—²ï¼Œåœ¨ free_block_ids ä¸­
        # = 1: è¢«ä¸€ä¸ª Sequence ç‹¬å ä½¿ç”¨
        # > 1: è¢«å¤šä¸ª Sequence å…±äº«ï¼ˆPrefix Caching åœºæ™¯ï¼‰
        self.ref_count = 0
        
        # hash: Block å†…å®¹çš„ xxhash å€¼ï¼Œç”¨äº Prefix Caching å¿«é€ŸæŸ¥æ‰¾
        # = -1: Block æœªå¡«æ»¡ï¼Œæˆ–ä¸å‚ä¸ç¼“å­˜
        # != -1: Block å·²å¡«æ»¡ï¼Œå¯è¢«åç»­è¯·æ±‚å¤ç”¨
        self.hash = -1
        
        # token_ids: Block ä¸­å­˜å‚¨çš„ token åºåˆ—
        # ç”¨äº hash ç¢°æ’æ—¶çš„ç²¾ç¡®æ ¡éªŒï¼Œç¡®ä¿å†…å®¹çœŸæ­£ç›¸åŒ
        self.token_ids = []

    def update(self, hash: int, token_ids: list[int]):
        """
        æ›´æ–° Block çš„ç¼“å­˜æ ‡è¯†ã€‚
        åªåœ¨ Block å¡«æ»¡ï¼ˆåŒ…å«å®Œæ•´ block_size ä¸ª tokenï¼‰æ—¶è°ƒç”¨ã€‚
        æ›´æ–°åï¼Œè¯¥ Block å¯è¢«åç»­å…·æœ‰ç›¸åŒå‰ç¼€çš„è¯·æ±‚å¤ç”¨ã€‚
        """
        self.hash = hash
        self.token_ids = token_ids

    def reset(self):
        """
        é‡ç½® Block çŠ¶æ€ï¼Œä¾›æ–°åˆ†é…ä½¿ç”¨ã€‚
        åœ¨ä» free_block_ids å–å‡ºå¹¶åˆ†é…ç»™æ–° Sequence æ—¶è°ƒç”¨ã€‚
        """
        self.ref_count = 1      # æ–°åˆ†é…ï¼Œåˆå§‹å¼•ç”¨è®¡æ•°ä¸º 1
        self.hash = -1          # æ¸…é™¤æ—§çš„ hashï¼ˆæ–°å†…å®¹å¾…å†™å…¥ï¼‰
        self.token_ids = []     # æ¸…é™¤æ—§çš„ token_ids
```

### 3.2 BlockManager ç±»ï¼ˆblock_manager.pyï¼‰

BlockManager æ˜¯ Block çš„ç®¡ç†å™¨ï¼Œè´Ÿè´£åˆ†é…ã€å›æ”¶å’Œ Prefix Cachingã€‚

```python
class BlockManager:
    """
    Block ç®¡ç†å™¨ï¼ŒPagedAttention çš„æ ¸å¿ƒç»„ä»¶ã€‚
    
    èŒè´£ï¼š
    1. ç®¡ç†ç‰©ç† Block çš„åˆ†é…å’Œå›æ”¶
    2. ç»´æŠ¤ç©ºé—² Block æ± 
    3. å®ç° Prefix Cachingï¼ˆé€šè¿‡ hash ç´¢å¼•å’Œå¼•ç”¨è®¡æ•°ï¼‰
    """

    def __init__(self, num_blocks: int, block_size: int):
        # block_size: æ¯ä¸ª Block å®¹çº³çš„ token æ•°ï¼Œé»˜è®¤ 256
        self.block_size = block_size
        
        # blocks: æ‰€æœ‰ç‰©ç† Block å®ä¾‹çš„åˆ—è¡¨
        # ç´¢å¼•å³ block_idï¼Œé•¿åº¦ä¸º num_blocks
        self.blocks: list[Block] = [Block(i) for i in range(num_blocks)]
        
        # hash_to_block_id: hash å€¼åˆ° block_id çš„æ˜ å°„
        # Prefix Caching çš„æ ¸å¿ƒç´¢å¼•ï¼Œç”¨äº O(1) æŸ¥æ‰¾æ˜¯å¦å­˜åœ¨ç›¸åŒå†…å®¹çš„ Block
        self.hash_to_block_id: dict[int, int] = dict()
        
        # free_block_ids: ç©ºé—² Block ID é˜Ÿåˆ—
        # ä½¿ç”¨ deque å®ç° FIFO åˆ†é…ç­–ç•¥
        self.free_block_ids: deque[int] = deque(range(num_blocks))
        
        # used_block_ids: å·²ä½¿ç”¨çš„ Block ID é›†åˆ
        # ç”¨äº O(1) åˆ¤æ–­æŸä¸ª Block æ˜¯å¦æ­£åœ¨è¢«ä½¿ç”¨
        self.used_block_ids: set[int] = set()

    @classmethod
    def compute_hash(cls, token_ids: list[int], prefix: int = -1):
        """
        è®¡ç®— Block å†…å®¹çš„ hash å€¼ã€‚
        
        ä½¿ç”¨é“¾å¼ hashï¼šå½“å‰ Block çš„ hash ä¾èµ–äºå‰ç¼€ Block çš„ hashã€‚
        è¿™ç¡®ä¿äº†åªæœ‰ã€Œå‰ç¼€å®Œå…¨ç›¸åŒã€çš„ Block åºåˆ—æ‰èƒ½åŒ¹é…ã€‚
        
        Args:
            token_ids: å½“å‰ Block çš„ token åˆ—è¡¨
            prefix: å‰ä¸€ä¸ª Block çš„ hash å€¼ï¼Œ-1 è¡¨ç¤ºè¿™æ˜¯ç¬¬ä¸€ä¸ª Block
        
        Returns:
            64 ä½æ•´æ•° hash å€¼
        """
        h = xxhash.xxh64()
        if prefix != -1:
            # å°†å‰ç¼€ hash çº³å…¥è®¡ç®—ï¼Œå®ç°é“¾å¼ä¾èµ–
            h.update(prefix.to_bytes(8, "little"))
        h.update(np.array(token_ids).tobytes())
        return h.intdigest()

    def _allocate_block(self, block_id: int) -> Block:
        """
        å†…éƒ¨æ–¹æ³•ï¼šå°†æŒ‡å®š Block ä»ç©ºé—²æ± ç§»åˆ°å·²ä½¿ç”¨é›†åˆã€‚
        """
        block = self.blocks[block_id]
        assert block.ref_count == 0  # ç¡®ä¿ Block ç¡®å®æ˜¯ç©ºé—²çš„
        block.reset()                # é‡ç½®çŠ¶æ€
        self.free_block_ids.remove(block_id)
        self.used_block_ids.add(block_id)
        return self.blocks[block_id]

    def _deallocate_block(self, block_id: int) -> Block:
        """
        å†…éƒ¨æ–¹æ³•ï¼šå°†æŒ‡å®š Block ä»å·²ä½¿ç”¨é›†åˆç§»å›ç©ºé—²æ± ã€‚
        æ³¨æ„ï¼šä¸æ¸…é™¤ hash å’Œ token_idsï¼Œä»¥ä¾¿åç»­å¯èƒ½çš„ç¼“å­˜å‘½ä¸­ã€‚
        """
        assert self.blocks[block_id].ref_count == 0
        self.used_block_ids.remove(block_id)
        self.free_block_ids.append(block_id)  # æ”¾åˆ°é˜Ÿå°¾ï¼ŒFIFO

    def can_allocate(self, seq: Sequence) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„ç©ºé—² Block ä¸º Sequence åˆ†é…ã€‚
        
        è¿™æ˜¯ä¿å®ˆä¼°è®¡ï¼Œæœªè€ƒè™‘ Prefix Caching å‘½ä¸­å¯èƒ½å‡å°‘çš„éœ€æ±‚ã€‚
        å®é™…åˆ†é…æ—¶å¯èƒ½å› ç¼“å­˜å‘½ä¸­è€Œéœ€è¦æ›´å°‘çš„ Blockã€‚
        """
        return len(self.free_block_ids) >= seq.num_blocks

    def allocate(self, seq: Sequence):
        """
        ä¸º Sequence åˆ†é… Blockï¼Œæ ¸å¿ƒæ–¹æ³•ã€‚
        
        åŒ…å«å®Œæ•´çš„ Prefix Caching é€»è¾‘ï¼š
        1. éå† Sequence çš„æ¯ä¸ªé€»è¾‘ Block
        2. è®¡ç®—é“¾å¼ hashï¼ŒæŸ¥æ‰¾ç¼“å­˜
        3. ç¼“å­˜å‘½ä¸­åˆ™å¤ç”¨ï¼Œæœªå‘½ä¸­åˆ™æ–°åˆ†é…
        4. æ›´æ–° Sequence çš„ block_table
        
        è°ƒç”¨æ—¶æœºï¼šPrefill é˜¶æ®µï¼Œæ–°è¯·æ±‚å¼€å§‹æ‰§è¡Œæ—¶
        """
        assert not seq.block_table  # ç¡®ä¿æ˜¯æ–°è¯·æ±‚ï¼Œblock_table åº”ä¸ºç©º
        h = -1                       # å‰ç¼€ hashï¼Œç”¨äºé“¾å¼è®¡ç®—
        cache_miss = False           # ä¸€æ—¦å‘ç”Ÿ missï¼Œåç»­éƒ½æ˜¯ miss
        
        for i in range(seq.num_blocks):
            token_ids = seq.block(i)  # è·å–ç¬¬ i ä¸ªé€»è¾‘ Block çš„ token
            
            # åªæœ‰å®Œæ•´ Blockï¼ˆåŒ…å« block_size ä¸ª tokenï¼‰æ‰è®¡ç®— hash
            # æœ€åä¸€ä¸ªæœªå¡«æ»¡çš„ Block ä¸å‚ä¸ç¼“å­˜
            h = self.compute_hash(token_ids, h) if len(token_ids) == self.block_size else -1
            
            # åœ¨ç¼“å­˜ç´¢å¼•ä¸­æŸ¥æ‰¾
            block_id = self.hash_to_block_id.get(h, -1)
            
            # åŒé‡æ ¡éªŒï¼šhash åŒ¹é… + å†…å®¹åŒ¹é…
            # é˜²æ­¢ hash ç¢°æ’æˆ– Block è¢«è¦†å†™å¯¼è‡´çš„é”™è¯¯å‘½ä¸­
            if block_id == -1 or self.blocks[block_id].token_ids != token_ids:
                cache_miss = True
            
            if cache_miss:
                # Cache Missï¼šä»ç©ºé—²æ± åˆ†é…æ–° Block
                block_id = self.free_block_ids[0]
                block = self._allocate_block(block_id)
            else:
                # Cache Hitï¼šå¤ç”¨å·²æœ‰ Block
                seq.num_cached_tokens += self.block_size  # ç´¯åŠ ç¼“å­˜å‘½ä¸­çš„ token æ•°
                if block_id in self.used_block_ids:
                    # Block æ­£è¢«å…¶ä»– Sequence ä½¿ç”¨ï¼Œå¢åŠ å¼•ç”¨è®¡æ•°
                    block = self.blocks[block_id]
                    block.ref_count += 1
                else:
                    # Block åœ¨ç©ºé—²æ± ä¸­ï¼ˆä¹‹å‰è¢«å›æ”¶ä½† hash ä¿ç•™ï¼‰ï¼Œé‡æ–°æ¿€æ´»
                    block = self._allocate_block(block_id)
            
            # æ›´æ–° Block çš„ hash å’Œ token_idsï¼ˆä»…å®Œæ•´ Blockï¼‰
            if h != -1:
                block.update(h, token_ids)
                self.hash_to_block_id[h] = block_id
            
            # å°† block_id åŠ å…¥ Sequence çš„ block_table
            seq.block_table.append(block_id)

    def deallocate(self, seq: Sequence):
        """
        é‡Šæ”¾ Sequence å ç”¨çš„æ‰€æœ‰ Blockã€‚
        
        é€šè¿‡å¼•ç”¨è®¡æ•°å®ç°ï¼š
        - ref_count å‡ 1
        - åªæœ‰å½“ ref_count é™ä¸º 0 æ—¶æ‰çœŸæ­£é‡Šæ”¾
        
        è°ƒç”¨æ—¶æœºï¼š
        1. è¯·æ±‚å®Œæˆï¼ˆpostprocess ä¸­æ£€æµ‹åˆ° EOS æˆ–è¾¾åˆ° max_tokensï¼‰
        2. è¯·æ±‚è¢«æŠ¢å ï¼ˆpreemptï¼‰
        """
        for block_id in reversed(seq.block_table):  # é€†åºéå†ï¼ˆæ ˆè¯­ä¹‰ï¼‰
            block = self.blocks[block_id]
            block.ref_count -= 1
            if block.ref_count == 0:
                self._deallocate_block(block_id)
        seq.num_cached_tokens = 0
        seq.block_table.clear()

    def can_append(self, seq: Sequence) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦èƒ½ä¸º Sequence è¿½åŠ æ–° tokenï¼ˆå¯èƒ½éœ€è¦æ–° Blockï¼‰ã€‚
        
        åªæœ‰å½“ len(seq) % block_size == 1 æ—¶ï¼ˆå³ä¸Šä¸€ä¸ª Block åˆšæ»¡ï¼‰ï¼Œ
        æ‰éœ€è¦åˆ†é…æ–° Blockã€‚å…¶ä»–æƒ…å†µç›´æ¥å†™å…¥ç°æœ‰ Blockã€‚
        
        è°ƒç”¨æ—¶æœºï¼šDecode é˜¶æ®µï¼Œæ¯æ¬¡è¿­ä»£å‰æ£€æŸ¥
        """
        return len(self.free_block_ids) >= (len(seq) % self.block_size == 1)

    def may_append(self, seq: Sequence):
        """
        Decode é˜¶æ®µè¿½åŠ  Block çš„å¤„ç†ã€‚
        
        ä¸‰ç§æƒ…å†µï¼š
        1. len % block_size == 1: éœ€è¦æ–° Blockï¼ˆä¸Šä¸€ä¸ªå·²æ»¡ï¼‰
        2. len % block_size == 0: å½“å‰ Block åˆšå¡«æ»¡ï¼Œæ›´æ–°å…¶ hash
        3. å…¶ä»–: Block æ­£åœ¨å¡«å……ä¸­ï¼Œæ— éœ€æ“ä½œ
        
        è°ƒç”¨æ—¶æœºï¼šDecode é˜¶æ®µï¼Œcan_append è¿”å› True åè°ƒç”¨
        """
        block_table = seq.block_table
        last_block = self.blocks[block_table[-1]]
        
        if len(seq) % self.block_size == 1:
            # æƒ…å†µ1ï¼šåˆšå¥½éœ€è¦æ–° Blockï¼ˆä¸Šä¸€ä¸ªå·²æ»¡ï¼‰
            assert last_block.hash != -1  # ä¸Šä¸€ä¸ª Block åº”è¯¥å·²ç»å®Œæ•´å¹¶æœ‰ hash
            block_id = self.free_block_ids[0]
            self._allocate_block(block_id)
            block_table.append(block_id)
            
        elif len(seq) % self.block_size == 0:
            # æƒ…å†µ2ï¼šå½“å‰ Block åˆšå¥½å¡«æ»¡ï¼Œæ›´æ–°å…¶ hashï¼ˆä¾›åç»­ Prefix Cacheï¼‰
            assert last_block.hash == -1  # ä¹‹å‰åº”è¯¥æ˜¯æœªå®ŒæˆçŠ¶æ€
            token_ids = seq.block(seq.num_blocks - 1)
            prefix = self.blocks[block_table[-2]].hash if len(block_table) > 1 else -1
            h = self.compute_hash(token_ids, prefix)
            last_block.update(h, token_ids)
            self.hash_to_block_id[h] = last_block.block_id
            
        else:
            # æƒ…å†µ3ï¼šBlock æ­£åœ¨å¡«å……ä¸­ï¼Œæ— éœ€æ“ä½œ
            assert last_block.hash == -1  # ç¡®è®¤æ˜¯æœªå®ŒæˆçŠ¶æ€
```

### 3.3 Sequence ç±»ï¼ˆsequence.pyï¼‰

Sequence ä»£è¡¨ä¸€ä¸ªæ¨ç†è¯·æ±‚ï¼ŒåŒ…å« token åºåˆ—å’Œ Block æ˜ å°„ä¿¡æ¯ã€‚

```python
class SequenceStatus(Enum):
    """Sequence çš„çŠ¶æ€æšä¸¾"""
    WAITING = auto()   # ç­‰å¾…è°ƒåº¦
    RUNNING = auto()   # æ­£åœ¨æ‰§è¡Œ
    FINISHED = auto()  # å·²å®Œæˆ


class Sequence:
    """
    æ¨ç†è¯·æ±‚çš„æŠ½è±¡ï¼ŒåŒ…å«è¾“å…¥ tokenã€ç”ŸæˆçŠ¶æ€å’Œ Block æ˜ å°„ã€‚
    
    æ ¸å¿ƒèŒè´£ï¼š
    1. å­˜å‚¨ prompt å’Œç”Ÿæˆçš„ token
    2. ç»´æŠ¤ block_tableï¼ˆé€»è¾‘ Block åˆ°ç‰©ç† Block çš„æ˜ å°„ï¼‰
    3. è®°å½• Prefix Caching å‘½ä¸­ä¿¡æ¯
    """
    
    # ç±»å˜é‡ï¼šæ‰€æœ‰ Sequence å…±äº«çš„ Block å¤§å°
    block_size = 256
    
    # ç±»å˜é‡ï¼šSequence ID ç”Ÿæˆå™¨ï¼Œç¡®ä¿æ¯ä¸ªè¯·æ±‚æœ‰å”¯ä¸€ ID
    counter = count()

    def __init__(self, token_ids: list[int], sampling_params=SamplingParams()):
        # seq_id: å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œç”¨äºç»“æœæ’åºå’Œè¿½è¸ª
        self.seq_id = next(Sequence.counter)
        
        # status: å½“å‰çŠ¶æ€ï¼ˆWAITING -> RUNNING -> FINISHEDï¼‰
        self.status = SequenceStatus.WAITING
        
        # token_ids: å®Œæ•´çš„ token åºåˆ—ï¼ˆprompt + å·²ç”Ÿæˆçš„ tokenï¼‰
        # ä½¿ç”¨ copy é¿å…å¤–éƒ¨ä¿®æ”¹å½±å“
        self.token_ids = copy(token_ids)
        
        # last_token: æœ€åä¸€ä¸ª tokenï¼ŒDecode é˜¶æ®µçš„è¾“å…¥
        self.last_token = token_ids[-1]
        
        # num_tokens: å½“å‰æ€» token æ•°ï¼ˆprompt + å·²ç”Ÿæˆï¼‰
        self.num_tokens = len(self.token_ids)
        
        # num_prompt_tokens: prompt çš„ token æ•°ï¼Œä¸å˜
        self.num_prompt_tokens = len(token_ids)
        
        # num_cached_tokens: Prefix Caching å‘½ä¸­çš„ token æ•°
        # ç”± BlockManager.allocate è®¾ç½®ï¼Œç”¨äºè·³è¿‡å·²ç¼“å­˜éƒ¨åˆ†çš„è®¡ç®—
        self.num_cached_tokens = 0
        
        # block_table: é€»è¾‘ Block ç´¢å¼• -> ç‰©ç† Block ID çš„æ˜ å°„
        # ä¾‹å¦‚ [7, 3, 12] è¡¨ç¤ºï¼š
        #   é€»è¾‘ Block 0 -> ç‰©ç† Block 7
        #   é€»è¾‘ Block 1 -> ç‰©ç† Block 3
        #   é€»è¾‘ Block 2 -> ç‰©ç† Block 12
        self.block_table = []
        
        # é‡‡æ ·å‚æ•°
        self.temperature = sampling_params.temperature
        self.max_tokens = sampling_params.max_tokens
        self.ignore_eos = sampling_params.ignore_eos

    def __len__(self):
        """è¿”å›å½“å‰ token æ€»æ•°"""
        return self.num_tokens

    def __getitem__(self, key):
        """æ”¯æŒåˆ‡ç‰‡è®¿é—® token_ids"""
        return self.token_ids[key]

    @property
    def is_finished(self):
        """æ˜¯å¦å·²å®Œæˆ"""
        return self.status == SequenceStatus.FINISHED

    @property
    def num_completion_tokens(self):
        """å·²ç”Ÿæˆçš„ token æ•°ï¼ˆä¸å« promptï¼‰"""
        return self.num_tokens - self.num_prompt_tokens

    @property
    def prompt_token_ids(self):
        """prompt éƒ¨åˆ†çš„ token"""
        return self.token_ids[:self.num_prompt_tokens]

    @property
    def completion_token_ids(self):
        """ç”Ÿæˆéƒ¨åˆ†çš„ token"""
        return self.token_ids[self.num_prompt_tokens:]

    @property
    def num_cached_blocks(self):
        """Prefix Caching å‘½ä¸­çš„ Block æ•°"""
        return self.num_cached_tokens // self.block_size

    @property
    def num_blocks(self):
        """
        å½“å‰éœ€è¦çš„ Block æ€»æ•°ã€‚
        å‘ä¸Šå–æ•´ï¼š(num_tokens + block_size - 1) // block_size
        """
        return (self.num_tokens + self.block_size - 1) // self.block_size

    @property
    def last_block_num_tokens(self):
        """
        æœ€åä¸€ä¸ª Block ä¸­çš„ token æ•°é‡ã€‚
        å¯èƒ½ä¸æ»¡ block_sizeï¼ˆæ­£åœ¨å¡«å……ä¸­ï¼‰ã€‚
        """
        return self.num_tokens - (self.num_blocks - 1) * self.block_size

    def block(self, i):
        """
        è·å–ç¬¬ i ä¸ªé€»è¾‘ Block å¯¹åº”çš„ token åˆ—è¡¨ã€‚
        ç”¨äºè®¡ç®— hash å’Œå†…å®¹æ ¡éªŒã€‚
        """
        assert 0 <= i < self.num_blocks
        return self.token_ids[i * self.block_size: (i + 1) * self.block_size]

    def append_token(self, token_id: int):
        """
        è¿½åŠ æ–°ç”Ÿæˆçš„ tokenã€‚
        åœ¨ Scheduler.postprocess ä¸­è°ƒç”¨ã€‚
        """
        self.token_ids.append(token_id)
        self.last_token = token_id
        self.num_tokens += 1

    def __getstate__(self):
        """
        åºåˆ—åŒ–æ”¯æŒï¼ˆç”¨äºå¤šè¿›ç¨‹é€šä¿¡ï¼‰ã€‚
        åªä¼ è¾“å¿…è¦çš„å­—æ®µï¼Œå‡å°‘é€šä¿¡å¼€é”€ã€‚
        """
        return (self.num_tokens, self.num_prompt_tokens, self.num_cached_tokens, 
                self.block_table,
                self.token_ids if self.num_completion_tokens == 0 else self.last_token)

    def __setstate__(self, state):
        """ååºåˆ—åŒ–"""
        self.num_tokens, self.num_prompt_tokens, self.num_cached_tokens, self.block_table = state[:-1]
        if self.num_completion_tokens == 0:
            self.token_ids = state[-1]
        else:
            self.last_token = state[-1]
```

**block_table æ˜ å°„å…³ç³»å›¾**ï¼š

![å›¾ 2](../.assets/57b930eeb3fef7b770c5bc635fffd785cefaa09726efb3613ddbab109f33aba0.png)  

### 3.4 KV Cache ç‰©ç†å­˜å‚¨ï¼ˆmodel_runner.py éƒ¨åˆ†ä»£ç ï¼‰

KV Cache çš„ç‰©ç†å­˜å‚¨ç”± `ModelRunner.allocate_kv_cache()` æ–¹æ³•åˆ›å»ºã€‚

```python
def allocate_kv_cache(self):
    """
    åˆ†é… KV Cache çš„ GPU æ˜¾å­˜ã€‚
    
    æ ¹æ®å¯ç”¨æ˜¾å­˜è‡ªåŠ¨è®¡ç®—å¯åˆ†é…çš„ Block æ•°é‡ï¼Œ
    ç„¶åé¢„åˆ†é…ä¸€ä¸ªå¤§çš„ Tensor ä½œä¸ºæ‰€æœ‰ Block çš„å­˜å‚¨ã€‚
    
    è°ƒç”¨æ—¶æœºï¼šModelRunner åˆå§‹åŒ–æ—¶ï¼Œæ¨¡å‹åŠ è½½å
    """
    config = self.config
    hf_config = config.hf_config
    
    # 1. è·å– GPU æ˜¾å­˜ä¿¡æ¯
    free, total = torch.cuda.mem_get_info()
    used = total - free
    # peak: æ¨¡å‹åŠ è½½å’Œé¢„çƒ­è¿‡ç¨‹ä¸­çš„å³°å€¼æ˜¾å­˜
    peak = torch.cuda.memory_stats()["allocated_bytes.all.peak"]
    # current: å½“å‰å·²åˆ†é…çš„æ˜¾å­˜
    current = torch.cuda.memory_stats()["allocated_bytes.all.current"]
    
    # 2. è®¡ç®—å•ä¸ª Block çš„æ˜¾å­˜å ç”¨
    # è€ƒè™‘å¼ é‡å¹¶è¡Œï¼šæ¯ä¸ª GPU åªå­˜å‚¨éƒ¨åˆ† KV å¤´
    num_kv_heads = hf_config.num_key_value_heads // self.world_size
    block_bytes = (2 *                              # K å’Œ V
                   hf_config.num_hidden_layers *    # å±‚æ•°ï¼ˆå¦‚ 28ï¼‰
                   self.block_size *                # block_sizeï¼ˆå¦‚ 256ï¼‰
                   num_kv_heads *                   # KV å¤´æ•°ï¼ˆå¦‚ 8ï¼‰
                   hf_config.head_dim *             # å¤´ç»´åº¦ï¼ˆå¦‚ 128ï¼‰
                   hf_config.torch_dtype.itemsize)  # æ•°æ®ç±»å‹å¤§å°ï¼ˆå¦‚ 2 for bf16ï¼‰
    
    # 3. è®¡ç®—å¯åˆ†é…çš„ Block æ•°é‡
    # å¯ç”¨æ˜¾å­˜ = æ€»æ˜¾å­˜ Ã— åˆ©ç”¨ç‡ - å·²ç”¨ - (å³°å€¼ - å½“å‰)
    available = total * config.gpu_memory_utilization - used - peak + current
    config.num_kvcache_blocks = int(available) // block_bytes
    assert config.num_kvcache_blocks > 0, "Not enough GPU memory for KV Cache"
    
    # 4. é¢„åˆ†é… KV Cache Tensor
    # å½¢çŠ¶: [2, num_layers, num_blocks, block_size, num_kv_heads, head_dim]
    self.kv_cache = torch.empty(
        2,                              # 0: Key, 1: Value
        hf_config.num_hidden_layers,    # å±‚æ•°
        config.num_kvcache_blocks,      # Block æ•°é‡
        self.block_size,                # æ¯ä¸ª Block çš„ token æ•°
        num_kv_heads,                   # KV å¤´æ•°
        hf_config.head_dim              # å¤´ç»´åº¦
    )
    
    # 5. å°† KV Cache åˆ‡ç‰‡ç»‘å®šåˆ°æ¯ä¸ª Attention å±‚
    # æ¯å±‚è·å¾— [num_blocks, block_size, num_kv_heads, head_dim] çš„è§†å›¾
    layer_id = 0
    for module in self.model.modules():
        if hasattr(module, "k_cache") and hasattr(module, "v_cache"):
            module.k_cache = self.kv_cache[0, layer_id]  # è¯¥å±‚çš„ K Cache
            module.v_cache = self.kv_cache[1, layer_id]  # è¯¥å±‚çš„ V Cache
            layer_id += 1
```

**KV Cache Tensor ç»“æ„å›¾**ï¼š

![å›¾ 3](../.assets/306dd6585d28e792e267ce164dbf97cd6755765b7dd15512b6d74107c55c93e7.png)  


### 3.5 Context ä¸Šä¸‹æ–‡ï¼ˆcontext.pyï¼‰

Context æ˜¯è¿è¡Œæ—¶ä¸Šä¸‹æ–‡çš„å®¹å™¨ï¼Œä¿å­˜ Attention è®¡ç®—æ‰€éœ€çš„å„ç§ç´¢å¼•ä¿¡æ¯ã€‚

```python
@dataclass
class Context:
    """
    è¿è¡Œæ—¶ä¸Šä¸‹æ–‡ï¼Œä¿å­˜å½“å‰è¿­ä»£çš„ Attention è®¡ç®—æ‰€éœ€ä¿¡æ¯ã€‚
    
    ä½¿ç”¨å…¨å±€å•ä¾‹æ¨¡å¼ï¼Œé€šè¿‡ set_context/get_context/reset_context è®¿é—®ã€‚
    è¿™æ · Attention å±‚å¯ä»¥æ— éœ€ä¿®æ”¹æ¥å£å³å¯è·å–è°ƒåº¦ä¿¡æ¯ã€‚
    """
    
    # is_prefill: å½“å‰æ˜¯å¦ä¸º Prefill é˜¶æ®µ
    # True: Prefillï¼Œå¤„ç†å®Œæ•´ prompt
    # False: Decodeï¼Œå¤„ç†å•ä¸ª token
    is_prefill: bool = False
    
    # cu_seqlens_q: Query çš„ç´¯ç§¯åºåˆ—é•¿åº¦ï¼ˆä»… Prefill ä½¿ç”¨ï¼‰
    # ä¾‹å¦‚ [0, 100, 250, 400] è¡¨ç¤º 3 ä¸ªåºåˆ—ï¼Œé•¿åº¦åˆ†åˆ«ä¸º 100, 150, 150
    # flash_attn_varlen_func éœ€è¦æ­¤å‚æ•°å¤„ç†å˜é•¿åºåˆ—æ‹¼æ¥
    cu_seqlens_q: torch.Tensor | None = None
    
    # cu_seqlens_k: Key çš„ç´¯ç§¯åºåˆ—é•¿åº¦ï¼ˆä»… Prefill ä½¿ç”¨ï¼‰
    # é€šå¸¸ä¸ cu_seqlens_q ç›¸åŒï¼Œä½† Prefix Cache æ—¶å¯èƒ½ä¸åŒ
    # ï¼ˆQuery åªåŒ…å«éç¼“å­˜ tokenï¼ŒKey åŒ…å«å…¨éƒ¨ï¼‰
    cu_seqlens_k: torch.Tensor | None = None
    
    # max_seqlen_q: æ‰¹æ¬¡ä¸­æœ€é•¿çš„ Query åºåˆ—é•¿åº¦ï¼ˆä»… Prefill ä½¿ç”¨ï¼‰
    max_seqlen_q: int = 0
    
    # max_seqlen_k: æ‰¹æ¬¡ä¸­æœ€é•¿çš„ Key åºåˆ—é•¿åº¦ï¼ˆä»… Prefill ä½¿ç”¨ï¼‰
    max_seqlen_k: int = 0
    
    # slot_mapping: token åˆ° KV Cache æ§½ä½çš„æ˜ å°„ï¼ˆPrefill å’Œ Decode éƒ½ä½¿ç”¨ï¼‰
    # Prefill: é•¿åº¦ = éç¼“å­˜ token æ€»æ•°
    # Decode: é•¿åº¦ = batch_sizeï¼ˆæ¯ä¸ªåºåˆ—ä¸€ä¸ªæ–° tokenï¼‰
    # æ§½ä½è®¡ç®—: block_id * block_size + offset_in_block
    slot_mapping: torch.Tensor | None = None
    
    # context_lens: æ¯ä¸ªåºåˆ—çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆä»… Decode ä½¿ç”¨ï¼‰
    # flash_attn_with_kvcache éœ€è¦çŸ¥é“æ¯ä¸ªåºåˆ—è¦è¯»å–å¤šå°‘å†å² KV
    context_lens: torch.Tensor | None = None
    
    # block_tables: æ‰¹é‡ block_tableï¼ˆDecode å’Œ Prefix Cache æ—¶ä½¿ç”¨ï¼‰
    # å½¢çŠ¶: [batch_size, max_blocks]
    # æ¯è¡Œæ˜¯ä¸€ä¸ªåºåˆ—çš„ block_tableï¼Œä¸è¶³éƒ¨åˆ†å¡«å…… -1
    block_tables: torch.Tensor | None = None


# å…¨å±€ä¸Šä¸‹æ–‡å®ä¾‹
_CONTEXT = Context()


def get_context():
    """è·å–å½“å‰ä¸Šä¸‹æ–‡ï¼ˆåœ¨ Attention.forward ä¸­è°ƒç”¨ï¼‰"""
    return _CONTEXT


def set_context(is_prefill, cu_seqlens_q=None, cu_seqlens_k=None, 
                max_seqlen_q=0, max_seqlen_k=0, slot_mapping=None, 
                context_lens=None, block_tables=None):
    """
    è®¾ç½®ä¸Šä¸‹æ–‡ï¼ˆåœ¨ ModelRunner.prepare_prefill/prepare_decode ä¸­è°ƒç”¨ï¼‰
    """
    global _CONTEXT
    _CONTEXT = Context(is_prefill, cu_seqlens_q, cu_seqlens_k, 
                       max_seqlen_q, max_seqlen_k, slot_mapping, 
                       context_lens, block_tables)


def reset_context():
    """é‡ç½®ä¸Šä¸‹æ–‡ï¼ˆåœ¨ ModelRunner.run ç»“æŸæ—¶è°ƒç”¨ï¼‰"""
    global _CONTEXT
    _CONTEXT = Context()
```

### 3.6 Attention å±‚çš„ KV Cache æ“ä½œï¼ˆattention.pyï¼‰

```python
@triton.jit
def store_kvcache_kernel(
    key_ptr,            # è¾“å…¥ K å¼ é‡çš„æŒ‡é’ˆ
    key_stride,         # K å¼ é‡åœ¨ token ç»´åº¦çš„æ­¥é•¿
    value_ptr,          # è¾“å…¥ V å¼ é‡çš„æŒ‡é’ˆ
    value_stride,       # V å¼ é‡åœ¨ token ç»´åº¦çš„æ­¥é•¿
    k_cache_ptr,        # K Cache å¼ é‡çš„æŒ‡é’ˆ
    v_cache_ptr,        # V Cache å¼ é‡çš„æŒ‡é’ˆ
    slot_mapping_ptr,   # slot_mapping çš„æŒ‡é’ˆ
    D: tl.constexpr,    # æ¯ä¸ª token çš„ KV ç»´åº¦ (num_heads * head_dim)
):
    """
    Triton Kernelï¼šå°† Kã€V å‘é‡å†™å…¥ KV Cache çš„æŒ‡å®šæ§½ä½ã€‚
    
    ä¸ºä»€ä¹ˆç”¨ Triton è€Œé PyTorchï¼š
    1. slot_mapping æŒ‡å®šçš„ä½ç½®ä¸è¿ç»­ï¼ŒPyTorch ç´¢å¼•æ“ä½œæ•ˆç‡ä½
    2. Triton å¯ä»¥å¹¶è¡Œå¤„ç†æ‰€æœ‰ tokenï¼Œæ¯ä¸ª token ä¸€ä¸ªçº¿ç¨‹å—
    3. åˆå¹¶è¯»å†™ï¼Œå‡å°‘æ˜¾å­˜å¸¦å®½å‹åŠ›
    """
    # å½“å‰å¤„ç†çš„ token ç´¢å¼•ï¼ˆæ¯ä¸ªçº¿ç¨‹å—å¤„ç†ä¸€ä¸ª tokenï¼‰
    idx = tl.program_id(0)
    
    # è·å–ç›®æ ‡æ§½ä½
    slot = tl.load(slot_mapping_ptr + idx)
    
    # slot = -1 æ˜¯ CUDA Graph å¡«å……çš„æ— æ•ˆä½ç½®ï¼Œè·³è¿‡
    if slot == -1:
        return
    
    # ä»è¾“å…¥å¼ é‡åŠ è½½ K å’Œ V
    key_offsets = idx * key_stride + tl.arange(0, D)
    value_offsets = idx * value_stride + tl.arange(0, D)
    key = tl.load(key_ptr + key_offsets)
    value = tl.load(value_ptr + value_offsets)
    
    # å†™å…¥ Cache
    cache_offsets = slot * D + tl.arange(0, D)
    tl.store(k_cache_ptr + cache_offsets, key)
    tl.store(v_cache_ptr + cache_offsets, value)


def store_kvcache(key: torch.Tensor, value: torch.Tensor, 
                  k_cache: torch.Tensor, v_cache: torch.Tensor, 
                  slot_mapping: torch.Tensor):
    """
    Python å°è£…ï¼šè°ƒç”¨ Triton Kernel å†™å…¥ KV Cacheã€‚
    
    Args:
        key: å½“å‰è®¡ç®—çš„ Kï¼Œå½¢çŠ¶ [N, num_heads, head_dim]
        value: å½“å‰è®¡ç®—çš„ Vï¼Œå½¢çŠ¶ [N, num_heads, head_dim]
        k_cache: K Cacheï¼Œå½¢çŠ¶ [num_blocks, block_size, num_heads, head_dim]
        v_cache: V Cacheï¼Œå½¢çŠ¶åŒä¸Š
        slot_mapping: æ§½ä½æ˜ å°„ï¼Œå½¢çŠ¶ [N]
    """
    N, num_heads, head_dim = key.shape
    D = num_heads * head_dim
    # éªŒè¯å¼ é‡å¸ƒå±€
    assert key.stride(-1) == 1 and value.stride(-1) == 1
    assert key.stride(1) == head_dim and value.stride(1) == head_dim
    assert k_cache.stride(1) == D and v_cache.stride(1) == D
    assert slot_mapping.numel() == N
    # å¯åŠ¨ Kernelï¼Œæ¯ä¸ª token ä¸€ä¸ªçº¿ç¨‹å—
    store_kvcache_kernel[(N,)](
        key, key.stride(0), 
        value, value.stride(0), 
        k_cache, v_cache, 
        slot_mapping, D
    )


class Attention(nn.Module):
    """
    Attention å±‚ï¼Œé›†æˆ KV Cache çš„è¯»å†™å’Œ Attention è®¡ç®—ã€‚
    
    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    1. Prefill: ä½¿ç”¨ flash_attn_varlen_func å¤„ç†å˜é•¿åºåˆ—
    2. Decode: ä½¿ç”¨ flash_attn_with_kvcache å¤„ç†å• token
    """

    def __init__(self, num_heads, head_dim, scale, num_kv_heads):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.scale = scale
        self.num_kv_heads = num_kv_heads
        # k_cache å’Œ v_cache åœ¨ ModelRunner.allocate_kv_cache ä¸­ç»‘å®š
        self.k_cache = self.v_cache = torch.tensor([])

    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):
        """
        æ‰§è¡Œ Attention è®¡ç®—ã€‚
        
        Args:
            q: Queryï¼Œå½¢çŠ¶ [N, num_heads, head_dim]
            k: Keyï¼Œå½¢çŠ¶ [N, num_kv_heads, head_dim]
            v: Valueï¼Œå½¢çŠ¶åŒä¸Š
        
        Returns:
            è¾“å‡ºï¼Œå½¢çŠ¶åŒ q
        """
        # è·å–å½“å‰ä¸Šä¸‹æ–‡
        context = get_context()
        k_cache, v_cache = self.k_cache, self.v_cache
        
        # 1. å°†å½“å‰è®¡ç®—çš„ Kã€V å†™å…¥ Cache
        if k_cache.numel() and v_cache.numel():
            store_kvcache(k, v, k_cache, v_cache, context.slot_mapping)
        
        # 2. æ ¹æ®é˜¶æ®µé€‰æ‹© Attention è®¡ç®—æ–¹å¼
        if context.is_prefill:
            # Prefill é˜¶æ®µ
            if context.block_tables is not None:
                # Prefix Cache å‘½ä¸­ï¼šä» Cache è¯»å–å†å² KV
                # ä¼ å…¥çš„ k, v åªåŒ…å«æ–°è®¡ç®—çš„ token
                # flash_attn å†…éƒ¨ä¼šæ ¹æ® block_table æ‹¼æ¥å†å² KV
                k, v = k_cache, v_cache
            
            o = flash_attn_varlen_func(
                q, k, v,
                max_seqlen_q=context.max_seqlen_q,
                cu_seqlens_q=context.cu_seqlens_q,
                max_seqlen_k=context.max_seqlen_k,
                cu_seqlens_k=context.cu_seqlens_k,
                softmax_scale=self.scale,
                causal=True,
                block_table=context.block_tables
            )
        else:
            # Decode é˜¶æ®µ
            # q: [batch_size, num_heads, head_dim] -> [batch_size, 1, num_heads, head_dim]
            o = flash_attn_with_kvcache(
                q.unsqueeze(1),
                k_cache, v_cache,
                cache_seqlens=context.context_lens,  # æ¯ä¸ªåºåˆ—çš„å†å²é•¿åº¦
                block_table=context.block_tables,     # å®šä½ Cache ä¸­çš„ KV
                softmax_scale=self.scale,
                causal=True
            )
        return o
```

### 3.7 Scheduler è°ƒåº¦å™¨ï¼ˆscheduler.pyï¼‰

```python
class Scheduler:
    """
    è°ƒåº¦å™¨ï¼Œå†³å®šæ¯æ¬¡è¿­ä»£æ‰§è¡Œå“ªäº› Sequenceã€‚
    
    æ ¸å¿ƒèŒè´£ï¼š
    1. ç®¡ç† waiting å’Œ running é˜Ÿåˆ—
    2. è°ƒç”¨ BlockManager åˆ†é…/å›æ”¶ Block
    3. å®ç° Prefill ä¼˜å…ˆå’ŒæŠ¢å æœºåˆ¶
    """

    def __init__(self, config: Config):
        self.max_num_seqs = config.max_num_seqs
        self.max_num_batched_tokens = config.max_num_batched_tokens
        self.eos = config.eos
        
        # BlockManager å®ä¾‹
        self.block_manager = BlockManager(config.num_kvcache_blocks, 
                                          config.kvcache_block_size)
        
        # waiting: ç­‰å¾…æ‰§è¡Œçš„æ–°è¯·æ±‚é˜Ÿåˆ—
        self.waiting: deque[Sequence] = deque()
        
        # running: æ­£åœ¨æ‰§è¡Œçš„è¯·æ±‚é˜Ÿåˆ—ï¼ˆå·²å®Œæˆ Prefillï¼Œåœ¨ Decodeï¼‰
        self.running: deque[Sequence] = deque()

    def is_finished(self):
        """æ£€æŸ¥æ˜¯å¦æ‰€æœ‰è¯·æ±‚éƒ½å·²å®Œæˆ"""
        return not self.waiting and not self.running

    def add(self, seq: Sequence):
        """æ·»åŠ æ–°è¯·æ±‚åˆ°ç­‰å¾…é˜Ÿåˆ—"""
        self.waiting.append(seq)

    def schedule(self) -> tuple[list[Sequence], bool]:
        """
        è°ƒåº¦æ–¹æ³•ï¼Œé€‰æ‹©æœ¬æ¬¡è¿­ä»£è¦æ‰§è¡Œçš„ Sequenceã€‚
        
        è°ƒåº¦ç­–ç•¥ï¼š
        1. Prefill ä¼˜å…ˆï¼šå…ˆå¤„ç† waiting é˜Ÿåˆ—ä¸­çš„æ–°è¯·æ±‚
        2. èµ„æºä¸è¶³æ—¶ï¼šPrefill è·³è¿‡ç­‰å¾…ï¼ŒDecode æŠ¢å 
        
        Returns:
            (scheduled_seqs, is_prefill): è¢«è°ƒåº¦çš„åºåˆ—åˆ—è¡¨å’Œæ˜¯å¦ä¸º Prefill é˜¶æ®µ
        """
        scheduled_seqs = []
        num_seqs = 0
        num_batched_tokens = 0
        
        # ========== Prefill è°ƒåº¦ ==========
        while self.waiting and num_seqs < self.max_num_seqs:
            seq = self.waiting[0]
            
            # æ£€æŸ¥ token æ•°é‡é™åˆ¶
            if num_batched_tokens + len(seq) > self.max_num_batched_tokens:
                break
            
            # æ£€æŸ¥ Block æ˜¯å¦è¶³å¤Ÿ
            if not self.block_manager.can_allocate(seq):
                break
            
            # åˆ†é… Block
            num_seqs += 1
            self.block_manager.allocate(seq)
            
            # æ›´æ–° token è®¡æ•°ï¼ˆåªè®¡ç®—éç¼“å­˜ tokenï¼‰
            num_batched_tokens += len(seq) - seq.num_cached_tokens
            
            # çŠ¶æ€è½¬ç§»ï¼šWAITING -> RUNNING
            seq.status = SequenceStatus.RUNNING
            self.waiting.popleft()
            self.running.append(seq)
            scheduled_seqs.append(seq)
        
        # å¦‚æœæœ‰ Prefill è¯·æ±‚ï¼Œç›´æ¥è¿”å›
        if scheduled_seqs:
            return scheduled_seqs, True  # is_prefill = True
        
        # ========== Decode è°ƒåº¦ ==========
        while self.running and num_seqs < self.max_num_seqs:
            seq = self.running.popleft()
            
            # æ£€æŸ¥æ˜¯å¦èƒ½è¿½åŠ æ–° token
            while not self.block_manager.can_append(seq):
                # èµ„æºä¸è¶³ï¼Œéœ€è¦æŠ¢å 
                if self.running:
                    # æŠ¢å æœ€åè¿›å…¥çš„è¯·æ±‚ï¼ˆLIFO ç­–ç•¥ï¼‰
                    self.preempt(self.running.pop())
                else:
                    # æ— æ³•ç»§ç»­ï¼ŒæŠ¢å è‡ªå·±
                    self.preempt(seq)
                    break
            else:
                # å¯ä»¥ç»§ç»­æ‰§è¡Œ
                num_seqs += 1
                self.block_manager.may_append(seq)
                scheduled_seqs.append(seq)
        
        # å°†è°ƒåº¦çš„åºåˆ—æ”¾å›é˜Ÿé¦–
        assert scheduled_seqs
        self.running.extendleft(reversed(scheduled_seqs))
        return scheduled_seqs, False  # is_prefill = False

    def preempt(self, seq: Sequence):
        """
        æŠ¢å ï¼šæš‚åœä¸€ä¸ªæ­£åœ¨æ‰§è¡Œçš„ Sequenceï¼Œé‡Šæ”¾å…¶èµ„æºã€‚
        
        è¢«æŠ¢å çš„ Sequence ä¼šå›åˆ° waiting é˜Ÿåˆ—å¤´éƒ¨ï¼Œ
        ç­‰å¾…èµ„æºå¯ç”¨æ—¶ä¼˜å…ˆæ¢å¤æ‰§è¡Œã€‚
        """
        seq.status = SequenceStatus.WAITING
        self.block_manager.deallocate(seq)  # é‡Šæ”¾æ‰€æœ‰ Block
        self.waiting.appendleft(seq)         # æ”¾åˆ°ç­‰å¾…é˜Ÿåˆ—å¤´éƒ¨

    def postprocess(self, seqs: list[Sequence], token_ids: list[int]):
        """
        åå¤„ç†ï¼šæ›´æ–° Sequence çŠ¶æ€ï¼Œæ£€æŸ¥ç»ˆæ­¢æ¡ä»¶ã€‚
        
        åœ¨æ¯æ¬¡è¿­ä»£åè°ƒç”¨ï¼Œå¤„ç†é‡‡æ ·ç»“æœã€‚
        """
        for seq, token_id in zip(seqs, token_ids):
            # è¿½åŠ æ–° token
            seq.append_token(token_id)
            
            # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
            is_eos = not seq.ignore_eos and token_id == self.eos
            is_max_tokens = seq.num_completion_tokens == seq.max_tokens
            
            if is_eos or is_max_tokens:
                seq.status = SequenceStatus.FINISHED
                self.block_manager.deallocate(seq)  # é‡Šæ”¾ Block
                self.running.remove(seq)
```

---

## å››ã€Prefix Caching æœºåˆ¶è¯¦è§£

### 4.1 è®¾è®¡åŠ¨æœº

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¤§é‡è¯·æ±‚å…±äº«ç›¸åŒçš„å‰ç¼€ï¼ˆå¦‚ System Promptï¼‰ï¼š

```
è¯·æ±‚1: [System Prompt] + "What is AI?"
è¯·æ±‚2: [System Prompt] + "Explain ML"
è¯·æ±‚3: [System Prompt] + "Write code"
```

å¦‚æœæ¯ä¸ªè¯·æ±‚éƒ½é‡æ–°è®¡ç®— System Prompt çš„ KV Cacheï¼Œä¼šæµªè´¹å¤§é‡è®¡ç®—å’Œæ˜¾å­˜ã€‚**Prefix Caching** å…è®¸è¿™äº›è¯·æ±‚å…±äº«ç›¸åŒå‰ç¼€çš„ Blockã€‚

### 4.2 é“¾å¼ Hash è®¡ç®—

nano-vllm ä½¿ç”¨ **é“¾å¼ hash** ç¡®ä¿åªæœ‰å®Œå…¨ç›¸åŒçš„å‰ç¼€æ‰èƒ½åŒ¹é…ã€‚

```python
# ===== BlockManager.compute_hash() =====

@classmethod
def compute_hash(cls, token_ids: list[int], prefix: int = -1):
    """
    è®¡ç®— Block å†…å®¹çš„ hash å€¼ã€‚
    
    å…³é”®ï¼šå½“å‰ Block çš„ hash ä¾èµ–äºå‰ç¼€ Block çš„ hashï¼Œ
    è¿™æ ·å³ä½¿ä¸¤ä¸ª Block å†…å®¹ç›¸åŒï¼Œå¦‚æœå‰ç¼€ä¸åŒï¼Œhash ä¹Ÿä¸åŒã€‚
    """
    h = xxhash.xxh64()
    if prefix != -1:
        # å°†å‰ç¼€ hash çº³å…¥è®¡ç®—
        h.update(prefix.to_bytes(8, "little"))
    h.update(np.array(token_ids).tobytes())
    return h.intdigest()
```

**é“¾å¼ Hash ç¤ºæ„å›¾**ï¼š

![å›¾ 18](../.assets/84a91c45b593821e028c036bb6450ee30e88acdc88166066a47750a402307207.png)  

### 4.3 ç¼“å­˜åŒ¹é…ä¸å¤ç”¨

åœ¨ `allocate` ä¸­ï¼Œå¯¹æ¯ä¸ª Block è¿›è¡Œç¼“å­˜æŸ¥æ‰¾å’ŒåŒé‡æ ¡éªŒï¼š

```python
# ===== allocate() ä¸­çš„ç¼“å­˜åŒ¹é…é€»è¾‘ =====

for i in range(seq.num_blocks):
    token_ids = seq.block(i)
    
    # åªæœ‰å®Œæ•´ Block æ‰è®¡ç®— hash
    if len(token_ids) == self.block_size:
        h = self.compute_hash(token_ids, h)  # h æ˜¯å‰ä¸€ä¸ª Block çš„ hash
    else:
        h = -1
    
    # æŸ¥æ‰¾ç¼“å­˜
    block_id = self.hash_to_block_id.get(h, -1)
    
    # ğŸ‘‡ åŒé‡æ ¡éªŒï¼šhash åŒ¹é… + å†…å®¹åŒ¹é…
    if block_id == -1 or self.blocks[block_id].token_ids != token_ids:
        cache_miss = True  # Miss åï¼Œåç»­å…¨éƒ¨ Miss
    
    if not cache_miss:
        # Cache Hit
        seq.num_cached_tokens += self.block_size
        block = self.blocks[block_id]
        block.ref_count += 1  # å¼•ç”¨è®¡æ•° +1
        # ä¸éœ€è¦ä» free_block_ids åˆ†é…
    else:
        # Cache Miss
        block_id = self.free_block_ids[0]
        block = self._allocate_block(block_id)
```

**ä¸ºä»€ä¹ˆéœ€è¦åŒé‡æ ¡éªŒï¼Ÿ**
1. **Hash ç¢°æ’**ï¼šxxhash ç¢°æ’æ¦‚ç‡æä½ä½†ä¸ä¸ºé›¶
2. **Block è¢«è¦†å†™**ï¼šBlock å›æ”¶åé‡æ–°åˆ†é…ç»™æ–°å†…å®¹ï¼Œæ—§çš„ `hash_to_block_id` æ˜ å°„å¯èƒ½æœªæ¸…é™¤

**å¼•ç”¨è®¡æ•°æœºåˆ¶**ï¼š

![å›¾ 19](../.assets/9f0895c05713900fce7eafc53c7dea979847020407f94899fb89e37c13d53f44.png)  

### 4.4 å®Œæ•´ç¤ºä¾‹ï¼šä¸¤ä¸ªè¯·æ±‚å…±äº«å‰ç¼€

![å›¾ 20](../.assets/838198e5e32a4a8b1c1a33cd79e22eb111fdb1db5ed341ca6ef4cd4cc6eafd7d.png)  

## äº”ã€Prefill æµç¨‹å…¨è§£æ

### 5.1 æµç¨‹æ¦‚è¿°

Prefill é˜¶æ®µå¤„ç†æ–°è¯·æ±‚çš„ promptï¼Œä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰ prompt token çš„ KV Cache å¹¶ç”Ÿæˆç¬¬ä¸€ä¸ª tokenã€‚

![å›¾ 5](../.assets/2b13dc27a908104b58e2fa16e5e677a72697d155eb2489b872bb2f9265b4e89b.png)  


### 5.2 Step 1ï¼šè°ƒåº¦å™¨é€‰æ‹©è¯·æ±‚

Scheduler ä» waiting é˜Ÿåˆ—ä¸­å–å‡ºæ–°è¯·æ±‚ï¼Œæ£€æŸ¥èµ„æºåè°ƒç”¨ BlockManager åˆ†é… Blockã€‚

```python
# ===== Scheduler.schedule() ä¸­çš„ Prefill è°ƒåº¦éƒ¨åˆ† =====

scheduled_seqs = []
num_seqs = 0
num_batched_tokens = 0

# éå†ç­‰å¾…é˜Ÿåˆ—
while self.waiting and num_seqs < self.max_num_seqs:
    seq = self.waiting[0]
    
    # æ£€æŸ¥1ï¼štoken æ•°é‡æ˜¯å¦è¶…è¿‡å•æ¬¡è¿­ä»£é™åˆ¶
    if num_batched_tokens + len(seq) > self.max_num_batched_tokens:
        break
    
    # æ£€æŸ¥2ï¼šæ˜¯å¦æœ‰è¶³å¤Ÿçš„ç©ºé—² Block
    if not self.block_manager.can_allocate(seq):
        break
    
    # é€šè¿‡æ£€æŸ¥ï¼Œå¼€å§‹åˆ†é…
    num_seqs += 1
    self.block_manager.allocate(seq)  # è°ƒç”¨ BlockManager åˆ†é… Block
    
    # ç»Ÿè®¡å®é™…éœ€è¦è®¡ç®—çš„ token æ•°ï¼ˆæ’é™¤ç¼“å­˜å‘½ä¸­çš„ï¼‰
    num_batched_tokens += len(seq) - seq.num_cached_tokens
    
    # çŠ¶æ€è½¬ç§»ï¼šWAITING -> RUNNING
    seq.status = SequenceStatus.RUNNING
    self.waiting.popleft()
    self.running.append(seq)
    scheduled_seqs.append(seq)

if scheduled_seqs:
    return scheduled_seqs, True  # is_prefill = True
```

**å…³é”®ç‚¹**ï¼š
- `can_allocate(seq)` ä¿å®ˆä¼°è®¡ï¼Œæ£€æŸ¥ `free_block_ids >= seq.num_blocks`
- `allocate(seq)` æ‰§è¡Œå®é™…åˆ†é…ï¼Œå†…éƒ¨ä¼šå¤„ç† Prefix Caching
- `num_cached_tokens` æ˜¯ Prefix Cache å‘½ä¸­çš„ token æ•°ï¼Œç”± `allocate` å†…éƒ¨è®¾ç½®

### 5.3 Step 2ï¼šBlockManager åˆ†é… Block

è¿™æ˜¯ Prefill é˜¶æ®µæœ€æ ¸å¿ƒçš„æ­¥éª¤ï¼ŒåŒ…å«å®Œæ•´çš„ Prefix Caching é€»è¾‘ã€‚

```python
# ===== BlockManager.allocate(seq) =====

def allocate(self, seq: Sequence):
    assert not seq.block_table  # ç¡®ä¿æ˜¯æ–°è¯·æ±‚
    
    h = -1              # å‰ç¼€ hashï¼Œç”¨äºé“¾å¼è®¡ç®—
    cache_miss = False  # ä¸€æ—¦ missï¼Œåç»­å…¨éƒ¨ miss
    
    # éå† Sequence çš„æ¯ä¸ªé€»è¾‘ Block
    for i in range(seq.num_blocks):
        token_ids = seq.block(i)  # è·å–ç¬¬ i ä¸ª Block çš„ token
        
        # åªæœ‰å®Œæ•´ Block æ‰è®¡ç®— hashï¼ˆæœ€åä¸€ä¸ªå¯èƒ½ä¸æ»¡ï¼‰
        if len(token_ids) == self.block_size:
            h = self.compute_hash(token_ids, h)  # é“¾å¼ hash
        else:
            h = -1  # ä¸å®Œæ•´ï¼Œä¸å‚ä¸ç¼“å­˜
        
        # æŸ¥æ‰¾ç¼“å­˜
        block_id = self.hash_to_block_id.get(h, -1)
        
        # åŒé‡æ ¡éªŒï¼šhash åŒ¹é… + å†…å®¹åŒ¹é…
        if block_id == -1 or self.blocks[block_id].token_ids != token_ids:
            cache_miss = True
        
        if cache_miss:
            # Cache Missï¼šä»ç©ºé—²æ± åˆ†é…æ–° Block
            block_id = self.free_block_ids[0]
            block = self._allocate_block(block_id)
        else:
            # Cache Hitï¼šå¤ç”¨å·²æœ‰ Block
            seq.num_cached_tokens += self.block_size  # ç´¯åŠ ç¼“å­˜å‘½ä¸­æ•°
            if block_id in self.used_block_ids:
                block = self.blocks[block_id]
                block.ref_count += 1  # å¢åŠ å¼•ç”¨è®¡æ•°
            else:
                block = self._allocate_block(block_id)
        
        # æ›´æ–° Block çš„ hashï¼ˆä»…å®Œæ•´ Blockï¼‰
        if h != -1:
            block.update(h, token_ids)
            self.hash_to_block_id[h] = block_id
        
        # è®°å½•æ˜ å°„å…³ç³»
        seq.block_table.append(block_id)
```

**æ‰§è¡Œæ•ˆæœå›¾ç¤º**ï¼š

![å›¾ 7](../.assets/73b70477c95c94f4e592137d8c5e70320b42b326446c46d790289fa81f6cdf62.png)  

### 5.4 Step 3ï¼šæ„é€ è¿è¡Œæ—¶ä¸Šä¸‹æ–‡

ModelRunner æ ¹æ® block_table å’Œ num_cached_tokens æ„é€  Attention æ‰€éœ€çš„å‚æ•°ã€‚

```python
# ===== ModelRunner.prepare_prefill(seqs) =====

def prepare_prefill(self, seqs: list[Sequence]):
    input_ids = []
    positions = []
    cu_seqlens_q = [0]  # Query ç´¯ç§¯é•¿åº¦
    cu_seqlens_k = [0]  # Key ç´¯ç§¯é•¿åº¦
    slot_mapping = []
    
    for seq in seqs:
        seqlen = len(seq)
        
        # å…³é”®ï¼šåªå–éç¼“å­˜çš„ token
        input_ids.extend(seq[seq.num_cached_tokens:])
        positions.extend(list(range(seq.num_cached_tokens, seqlen)))
        
        # Query é•¿åº¦ = éç¼“å­˜ token æ•°
        seqlen_q = seqlen - seq.num_cached_tokens
        # Key é•¿åº¦ = å…¨éƒ¨ token æ•°ï¼ˆAttention éœ€è¦çœ‹åˆ°å®Œæ•´å†å²ï¼‰
        seqlen_k = seqlen
        
        cu_seqlens_q.append(cu_seqlens_q[-1] + seqlen_q)
        cu_seqlens_k.append(cu_seqlens_k[-1] + seqlen_k)

        # ä¸¾ä¾‹ï¼šå‡è®¾æœ‰ 2 ä¸ªåºåˆ—
        # - åºåˆ— A: [1,2,3] (3ä¸ªtoken, æ— ç¼“å­˜)
        # - åºåˆ— B: [4,5,6,7] (4ä¸ªtoken, æ— ç¼“å­˜)

        # cu_seqlens_q = [0, 3, 7]   # Aä»0å¼€å§‹é•¿åº¦3ï¼ŒBä»3å¼€å§‹é•¿åº¦4
        # cu_seqlens_k = [0, 3, 7]   # åŒæ ·
        # è¡¨ç¤º: åºåˆ—Aæ˜¯ indices[0:3], åºåˆ—Bæ˜¯ indices[3:7]
        
        # æ„é€  slot_mappingï¼šåªä¸ºéç¼“å­˜ Block çš„ token ç”Ÿæˆ
        for i in range(seq.num_cached_blocks, seq.num_blocks):
            block_id = seq.block_table[i]
            start = block_id * self.block_size
            if i != seq.num_blocks - 1:
                end = start + self.block_size
            else:
                end = start + seq.last_block_num_tokens
            slot_mapping.extend(list(range(start, end)))

        # ä¸¾ä¾‹ï¼šå‡è®¾ block_size=256
        # - åºåˆ— A åˆ†é…äº† 1 ä¸ª block (block_id=5)ï¼Œæœ‰ 100 ä¸ªæœ‰æ•ˆ token
        # - start = 5 * 256 = 1280
        # - end = 1280 + 100 = 1380
        # - slot_mapping æ·»åŠ  [1280, 1281, ..., 1379]

        # Slot Mapping çš„ä½œç”¨ï¼šå‘Šè¯‰ flash-attention æ¯ä¸ª token åº”è¯¥å†™å…¥ KV-cache çš„å“ªä¸ªä½ç½®ã€‚

    
    # å¦‚æœæœ‰ Prefix Cache å‘½ä¸­ï¼Œéœ€è¦ block_tables ä¾› Attention è¯»å–å†å² KV
    if cu_seqlens_k[-1] > cu_seqlens_q[-1]:
        block_tables = self.prepare_block_tables(seqs)
    else:
        block_tables = None
    
    # è½¬æ¢ä¸º Tensor å¹¶è®¾ç½®å…¨å±€ä¸Šä¸‹æ–‡
    set_context(
        is_prefill=True,
        cu_seqlens_q=torch.tensor(cu_seqlens_q).cuda(),
        cu_seqlens_k=torch.tensor(cu_seqlens_k).cuda(),
        slot_mapping=torch.tensor(slot_mapping).cuda(),
        block_tables=block_tables,
        # ...
    )
    return input_ids, positions
```

**slot_mapping æ„é€ å›¾ç¤º**ï¼š

![å›¾ 9](../.assets/dfced6a7d5dc290e71969381453ed9bb8b8b626aca3883f75b687096fb9d5ce5.png)  


### 5.5 Step 4ï¼šAttention è®¡ç®—ä¸ KV Cache å†™å…¥

Attention å±‚æ ¹æ® Context ä¸­çš„ä¿¡æ¯ï¼Œå†™å…¥ KV Cache å¹¶æ‰§è¡Œæ³¨æ„åŠ›è®¡ç®—ã€‚

```python
# ===== Attention.forward(q, k, v) =====

def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):
    context = get_context()
    
    # Step 4.1: å°†æ–°è®¡ç®—çš„ K, V å†™å…¥ Cache
    if self.k_cache.numel() and self.v_cache.numel():
        store_kvcache(k, v, self.k_cache, self.v_cache, context.slot_mapping)
    
    # Step 4.2: æ‰§è¡Œ Attention è®¡ç®—
    if context.is_prefill:
        # æ£€æŸ¥æ˜¯å¦æœ‰ Prefix Cache å‘½ä¸­
        if context.block_tables is not None:
            # æœ‰ç¼“å­˜å‘½ä¸­ï¼šä» Cache è¯»å–å†å² KV
            # flash_attn ä¼šæ ¹æ® block_table å®šä½å¹¶æ‹¼æ¥
            k, v = self.k_cache, self.v_cache
        
        o = flash_attn_varlen_func(
            q, k, v,
            cu_seqlens_q=context.cu_seqlens_q,  # Query ç´¯ç§¯é•¿åº¦
            cu_seqlens_k=context.cu_seqlens_k,  # Key ç´¯ç§¯é•¿åº¦ï¼ˆå«ç¼“å­˜ï¼‰
            max_seqlen_q=context.max_seqlen_q,
            max_seqlen_k=context.max_seqlen_k,
            softmax_scale=self.scale,
            causal=True,
            block_table=context.block_tables  # ä¼ é€’ block_table
        )
    else:
        # Decode é˜¶æ®µï¼ˆä¸‹ä¸€èŠ‚è®²è§£ï¼‰
        pass
    
    return o
```

**KV Cache å†™å…¥è¿‡ç¨‹**ï¼š

```python
@triton.jit
def store_kvcache_kernel(
    key_ptr,            # è¾“å…¥ K å¼ é‡çš„æŒ‡é’ˆ
    key_stride,         # K å¼ é‡åœ¨ token ç»´åº¦çš„æ­¥é•¿
    value_ptr,          # è¾“å…¥ V å¼ é‡çš„æŒ‡é’ˆ
    value_stride,       # V å¼ é‡åœ¨ token ç»´åº¦çš„æ­¥é•¿
    k_cache_ptr,        # K Cache å¼ é‡çš„æŒ‡é’ˆ
    v_cache_ptr,        # V Cache å¼ é‡çš„æŒ‡é’ˆ
    slot_mapping_ptr,   # slot_mapping çš„æŒ‡é’ˆ
    D: tl.constexpr,    # æ¯ä¸ª token çš„ KV ç»´åº¦ (num_heads * head_dim)
):
    """
    Triton Kernelï¼šå°† Kã€V å‘é‡å†™å…¥ KV Cache çš„æŒ‡å®šæ§½ä½ã€‚
    
    ä¸ºä»€ä¹ˆç”¨ Triton è€Œé PyTorchï¼š
    1. slot_mapping æŒ‡å®šçš„ä½ç½®ä¸è¿ç»­ï¼ŒPyTorch ç´¢å¼•æ“ä½œæ•ˆç‡ä½
    2. Triton å¯ä»¥å¹¶è¡Œå¤„ç†æ‰€æœ‰ tokenï¼Œæ¯ä¸ª token ä¸€ä¸ªçº¿ç¨‹å—
    3. åˆå¹¶è¯»å†™ï¼Œå‡å°‘æ˜¾å­˜å¸¦å®½å‹åŠ›
    """
    # å½“å‰å¤„ç†çš„ token ç´¢å¼•ï¼ˆæ¯ä¸ªçº¿ç¨‹å—å¤„ç†ä¸€ä¸ª tokenï¼‰
    idx = tl.program_id(0)
    
    # è·å–ç›®æ ‡æ§½ä½
    slot = tl.load(slot_mapping_ptr + idx)
    
    # slot = -1 æ˜¯ CUDA Graph å¡«å……çš„æ— æ•ˆä½ç½®ï¼Œè·³è¿‡
    if slot == -1:
        return
    
    # ä»è¾“å…¥å¼ é‡åŠ è½½ K å’Œ V
    key_offsets = idx * key_stride + tl.arange(0, D)
    value_offsets = idx * value_stride + tl.arange(0, D)
    key = tl.load(key_ptr + key_offsets)
    value = tl.load(value_ptr + value_offsets)
    
    # å†™å…¥ Cache
    cache_offsets = slot * D + tl.arange(0, D)
    tl.store(k_cache_ptr + cache_offsets, key)
    tl.store(v_cache_ptr + cache_offsets, value)


def store_kvcache(key: torch.Tensor, value: torch.Tensor, 
                  k_cache: torch.Tensor, v_cache: torch.Tensor, 
                  slot_mapping: torch.Tensor):
    """
    Python å°è£…ï¼šè°ƒç”¨ Triton Kernel å†™å…¥ KV Cacheã€‚
    
    Args:
        key: å½“å‰è®¡ç®—çš„ Kï¼Œå½¢çŠ¶ [N, num_heads, head_dim]
        value: å½“å‰è®¡ç®—çš„ Vï¼Œå½¢çŠ¶ [N, num_heads, head_dim]
        k_cache: K Cacheï¼Œå½¢çŠ¶ [num_blocks, block_size, num_heads, head_dim]
        v_cache: V Cacheï¼Œå½¢çŠ¶åŒä¸Š
        slot_mapping: æ§½ä½æ˜ å°„ï¼Œå½¢çŠ¶ [N]
    """
    N, num_heads, head_dim = key.shape
    D = num_heads * head_dim
    # éªŒè¯å¼ é‡å¸ƒå±€
    assert key.stride(-1) == 1 and value.stride(-1) == 1
    assert key.stride(1) == head_dim and value.stride(1) == head_dim
    assert k_cache.stride(1) == D and v_cache.stride(1) == D
    assert slot_mapping.numel() == N
    # å¯åŠ¨ Kernelï¼Œæ¯ä¸ª token ä¸€ä¸ªçº¿ç¨‹å—
    store_kvcache_kernel[(N,)](
        key, key.stride(0), 
        value, value.stride(0), 
        k_cache, v_cache, 
        slot_mapping, D
    )
```

![å›¾ 13](../.assets/81267e618a6e5e678ecf151ff77d50e1aff62f6926db8f2fe097e4f44f932b86.png)  


### 5.6 Step 5ï¼šåå¤„ç†

é‡‡æ ·ç”Ÿæˆ token åï¼ŒScheduler æ›´æ–° Sequence çŠ¶æ€ã€‚

```python
# ===== Scheduler.postprocess(seqs, token_ids) =====

def postprocess(self, seqs: list[Sequence], token_ids: list[int]):
    for seq, token_id in zip(seqs, token_ids):
        # è¿½åŠ æ–°ç”Ÿæˆçš„ token
        seq.append_token(token_id)
        
        # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
        is_eos = not seq.ignore_eos and token_id == self.eos
        is_max_tokens = seq.num_completion_tokens == seq.max_tokens
        
        if is_eos or is_max_tokens:
            # è¯·æ±‚å®Œæˆ
            seq.status = SequenceStatus.FINISHED
            self.block_manager.deallocate(seq)  # é‡Šæ”¾ Block
            self.running.remove(seq)
        # else: ç»§ç»­ç•™åœ¨ running é˜Ÿåˆ—ï¼Œä¸‹æ¬¡è¿›å…¥ Decode é˜¶æ®µ
```

**Prefill å®Œæˆåçš„çŠ¶æ€**ï¼š
- Sequence ä» `WAITING` å˜ä¸º `RUNNING`
- `block_table` å·²å¡«å……å®Œæ¯•
- KV Cache å·²å†™å…¥ï¼ˆé™¤ç¼“å­˜å‘½ä¸­éƒ¨åˆ†ï¼‰
- ç”Ÿæˆäº†ç¬¬ä¸€ä¸ª tokenï¼Œå‡†å¤‡è¿›å…¥ Decode é˜¶æ®µ

---

## å…­ã€Decode æµç¨‹å…¨è§£æ

### 6.1 æµç¨‹æ¦‚è¿°

Decode é˜¶æ®µé€ä¸ªç”Ÿæˆ tokenï¼Œæ¯æ¬¡è¿­ä»£åªå¤„ç†ä¸€ä¸ªæ–° tokenï¼Œä½†å¯ä»¥æ‰¹é‡å¤„ç†å¤šä¸ª Sequenceã€‚

![å›¾ 14](../.assets/d59cadf2568903052fa3bc6fbdde4e55925c3a6fdb3fecb47b5b8c0dce600de6.png)  


### 6.2 Step 1ï¼šè°ƒåº¦ä¸æŠ¢å 

Decode è°ƒåº¦çš„æ ¸å¿ƒæ˜¯å¤„ç†èµ„æºä¸è¶³æ—¶çš„æŠ¢å é€»è¾‘ã€‚

```python
# ===== Scheduler.schedule() ä¸­çš„ Decode è°ƒåº¦éƒ¨åˆ† =====

# ï¼ˆPrefill è°ƒåº¦è¿”å›ç©ºæ—¶ï¼Œè¿›å…¥ Decode è°ƒåº¦ï¼‰
while self.running and num_seqs < self.max_num_seqs:
    seq = self.running.popleft()
    
    # æ£€æŸ¥æ˜¯å¦èƒ½è¿½åŠ æ–° tokenï¼ˆå¯èƒ½éœ€è¦æ–° Blockï¼‰
    while not self.block_manager.can_append(seq):
        # èµ„æºä¸è¶³ï¼Œéœ€è¦æŠ¢å 
        if self.running:
            # æŠ¢å æœ€åè¿›å…¥çš„è¯·æ±‚ï¼ˆLIFO ç­–ç•¥ï¼‰
            victim = self.running.pop()
            self.preempt(victim)
        else:
            # æ²¡æœ‰å…¶ä»–è¯·æ±‚å¯æŠ¢å ï¼Œåªèƒ½æŠ¢å è‡ªå·±
            self.preempt(seq)
            break
    else:
        # èµ„æºå……è¶³ï¼Œå¯ä»¥ç»§ç»­
        num_seqs += 1
        self.block_manager.may_append(seq)
        scheduled_seqs.append(seq)

# å°†è°ƒåº¦çš„åºåˆ—æ”¾å›é˜Ÿé¦–ï¼ˆä¿æŒé¡ºåºï¼‰
self.running.extendleft(reversed(scheduled_seqs))
return scheduled_seqs, False  # is_prefill = False
```

**æŠ¢å å¤„ç†**ï¼š

```python
# ===== Scheduler.preempt(seq) =====

def preempt(self, seq: Sequence):
    # 1. çŠ¶æ€å›é€€
    seq.status = SequenceStatus.WAITING
    
    # 2. é‡Šæ”¾æ‰€æœ‰ Blockï¼ˆå½’è¿˜åˆ°ç©ºé—²æ± ï¼‰
    self.block_manager.deallocate(seq)
    
    # 3. æ”¾å›ç­‰å¾…é˜Ÿåˆ—å¤´éƒ¨ï¼ˆä¼˜å…ˆæ¢å¤ï¼‰
    self.waiting.appendleft(seq)
```

### 6.3 Step 2ï¼šæŒ‰éœ€è¿½åŠ  Block

may_append å¤„ç†ä¸‰ç§æƒ…å†µï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„æ§½ä½å­˜æ”¾æ–° tokenã€‚

```python
# ===== BlockManager.may_append(seq) =====

def may_append(self, seq: Sequence):
    block_table = seq.block_table
    last_block = self.blocks[block_table[-1]]
    
    if len(seq) % self.block_size == 1:
        # æƒ…å†µ1ï¼šéœ€è¦æ–° Blockï¼ˆä¸Šä¸€ä¸ªåˆšæ»¡ï¼Œæ–° token æ˜¯æ–° Block çš„ç¬¬ä¸€ä¸ªï¼‰
        # ä¾‹å¦‚ï¼šblock_size=256, len(seq)=257
        assert last_block.hash != -1  # ä¸Šä¸€ä¸ª Block åº”è¯¥å·²å®Œæ•´
        block_id = self.free_block_ids[0]
        self._allocate_block(block_id)
        block_table.append(block_id)
        
    elif len(seq) % self.block_size == 0:
        # æƒ…å†µ2ï¼šBlock åˆšå¥½å¡«æ»¡ï¼ˆæ–° token æ˜¯å½“å‰ Block çš„æœ€åä¸€ä¸ªï¼‰
        # ä¾‹å¦‚ï¼šblock_size=256, len(seq)=256
        assert last_block.hash == -1  # ä¹‹å‰åº”è¯¥æ˜¯æœªå®ŒæˆçŠ¶æ€
        token_ids = seq.block(seq.num_blocks - 1)
        prefix = self.blocks[block_table[-2]].hash if len(block_table) > 1 else -1
        h = self.compute_hash(token_ids, prefix)
        last_block.update(h, token_ids)  # æ›´æ–° hashï¼Œä¾›åç»­ç¼“å­˜
        self.hash_to_block_id[h] = last_block.block_id
        
    else:
        # æƒ…å†µ3ï¼šBlock æ­£åœ¨å¡«å……ä¸­ï¼Œæ— éœ€æ“ä½œ
        # ä¾‹å¦‚ï¼šblock_size=256, len(seq)=100
        assert last_block.hash == -1
```

**ä¸‰ç§æƒ…å†µå›¾ç¤º**ï¼š

![å›¾ 15](../.assets/59bbda88fa1eb374b5c026db53d9ee399f2cee23e4e9f4e0013fdab72007eebe.png)  


### 6.4 Step 3ï¼šæ„é€  Decode ä¸Šä¸‹æ–‡

Decode é˜¶æ®µæ¯ä¸ª Sequence åªæœ‰ä¸€ä¸ªæ–° tokenï¼Œæ„é€ è¿‡ç¨‹æ›´ç®€å•ã€‚

```python
# ===== ModelRunner.prepare_decode(seqs) =====

def prepare_decode(self, seqs: list[Sequence]):
    input_ids = []
    positions = []
    slot_mapping = []
    context_lens = []
    
    for seq in seqs:
        # åªå–æœ€åä¸€ä¸ª token
        input_ids.append(seq.last_token)
        positions.append(len(seq) - 1)
        
        # ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆAttention éœ€è¦çœ‹åˆ°çš„å†å²é•¿åº¦ï¼‰
        context_lens.append(len(seq))
        
        # è®¡ç®—æ–° token çš„ slot ä½ç½®
        block_id = seq.block_table[-1]
        offset = seq.last_block_num_tokens - 1  # åœ¨ Block å†…çš„åç§»
        slot = block_id * self.block_size + offset
        slot_mapping.append(slot)
    
    # æ„é€ æ‰¹é‡ block_tablesï¼ˆ2D Tensorï¼‰
    block_tables = self.prepare_block_tables(seqs)
    
    # è®¾ç½®ä¸Šä¸‹æ–‡
    set_context(
        is_prefill=False,
        slot_mapping=torch.tensor(slot_mapping).cuda(),
        context_lens=torch.tensor(context_lens).cuda(),
        block_tables=block_tables
    )
    return input_ids, positions
```

**Decode ä¸Šä¸‹æ–‡å›¾ç¤º**ï¼š

![å›¾ 16](../.assets/a811af35d7b8a8e2562b837e6666893503e7e7d093efb8cbae0a99d22634782c.png)  

### 6.5 Step 4ï¼šDecode Attention è®¡ç®—

Decode é˜¶æ®µä½¿ç”¨ `flash_attn_with_kvcache`ï¼Œä¸“ä¸ºå• token + Cache åœºæ™¯ä¼˜åŒ–ã€‚

```python
# ===== Attention.forward() ä¸­çš„ Decode åˆ†æ”¯ =====

def forward(self, q, k, v):
    context = get_context()
    
    # å†™å…¥å•ä¸ª token çš„ KV
    store_kvcache(k, v, self.k_cache, self.v_cache, context.slot_mapping)
    
    if not context.is_prefill:
        # Decode é˜¶æ®µ
        o = flash_attn_with_kvcache(
            q.unsqueeze(1),              # [batch, 1, heads, dim]
            self.k_cache,                 # æ•´ä¸ª K Cache
            self.v_cache,                 # æ•´ä¸ª V Cache
            cache_seqlens=context.context_lens,  # æ¯ä¸ª Seq çš„å†å²é•¿åº¦
            block_table=context.block_tables,     # å®šä½ Cache ä¸­çš„ KV
            softmax_scale=self.scale,
            causal=True
        )
        return o.squeeze(1)  # [batch, heads, dim]
```

**Decode Attention å·¥ä½œåŸç†**ï¼š

![å›¾ 17](../.assets/9d4eb1cfbe6d80f6ad847d23b684092d80a734991473e0b9bd80b0117d9d7f8d.png)  

---
